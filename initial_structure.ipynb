{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Thesis ~ Initial Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Types\n",
    "\n",
    " - Classification \n",
    " - Regression\n",
    " - Clustering\n",
    " - Dimension Reduction\n",
    " - Data Visualization & Analysis\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helpers\n",
    "import pprint\n",
    "from dateutil import parser\n",
    "import ijson\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Visions | Data Type Detection\n",
    "from visions.functional import detect_type, infer_type, cast_to_inferred\n",
    "from visions.typesets import StandardSet\n",
    "from visions.typesets import CompleteSet\n",
    "\n",
    "# YData Profiling\n",
    "from ydata_profiling import ProfileReport\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Problem - Clear Objectives/Requirements\n",
    "Problems before requirements, requirements before solutions, solutions before design, and design before technology.\n",
    "\n",
    "- Problem types | Classification, Regression, Clustering, Dimension Reduction, Data Visualizations & Techniques\n",
    "- Target\n",
    "- Models | Random Forest | Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem | {Problem Type, Target Variable}\n",
    "def df_objective (problem_type, target_variable):\n",
    "\n",
    "    return {\n",
    "        'problem_type' : problem_type,\n",
    "        'target_feature' : target_variable\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective | Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titanic\n",
    "tn_objective = df_objective('classification', 'Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering - Tabular Data\n",
    "\n",
    "**File Data Types**: json, csv, xlsx, xml, dataframe,\n",
    "\n",
    "**API**: url link, http, ftp\n",
    "\n",
    "**Category**: Web page, Image, Audio, Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Characteristics\n",
    "def file_descr (file_path): \n",
    "    descr = {\n",
    "        'FileType': os.path.splitext(file_path)[1],\n",
    "        'FileSize': os.path.getsize(file_path),\n",
    "    }\n",
    "    return descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converters | File type to Dataframe\n",
    "\n",
    "# Covert File to Dataframe\n",
    "def df_convert (file_path, file_descr):\n",
    "\n",
    "    # JSON\n",
    "    if (file_descr['FileType'] == '.json'):\n",
    "\n",
    "        # Open File and Convert it to JSON Object\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = ijson.items(file, 'item')\n",
    "            json_object = []\n",
    "            count = 0\n",
    "            print(data)\n",
    "            for line in data:\n",
    "                json_object.append(line)\n",
    "                count = count + 1\n",
    "                if(count == 1000): break\n",
    "        return pd.DataFrame(json_object)\n",
    "\n",
    "    # CSV\n",
    "    elif (file_descr['FileType'] == '.csv'):\n",
    "\n",
    "        # Check whether there is a header | TODO\n",
    "        return pd.read_csv(file_path)\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gather Data\n",
    "def df_gather(fp):\n",
    "\n",
    "    # File Characteristics\n",
    "    file_characteristics = file_descr(fp)\n",
    "\n",
    "    # Dataframe Initialization \n",
    "    df = df_convert(fp, file_characteristics)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering | Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wasabi\n",
    "df_ws = df_gather('data/sample_data/Wasabi/batterytesters_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# House Prices\n",
    "df_hp = df_gather('data/sample_data/house_price_data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit Card Fraud\n",
    "df_cc = df_gather('data/sample_data/credit_card_fraud/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modcloth | TOCHECK\n",
    "file_path_md = 'data/sample_data/modcloth/modcloth_final_data.json'\n",
    "df_md = pd.read_json(file_path_md, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titanic \n",
    "df_tn = df_gather('data/sample_data/titanic/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Profiling\n",
    "\n",
    "### Dataset\n",
    "- Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features, Rows\n",
    "def df_shape(df):\n",
    "    return {\n",
    "        'features': df.shape[1],\n",
    "        'rows': df.shape[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "- Role | ID, Input (Indpendant), Target (Dependant)\n",
    "- Data Type | Integer, Float, Boolean, Categorical, Complex, DateTime, Object, String\n",
    "- Feature Type | Categorical (Ordinal, Binary), Numerical\n",
    "- Level | Nominal, Interval, Ordinal, Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Type | Standard Set | Integer,  Float, Boolean, Categorical, Complex, DateTime, Object, String\n",
    "typeset = StandardSet()\n",
    "def f_data_type(f):\n",
    "    return str((detect_type(f, typeset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Type | Categorical, Numerical, Alphanumerical | {Data Type, Unique values, Thres}\n",
    "def f_feature_type(data_type, unique_values, threshold=10): \n",
    "\n",
    "    if ((data_type == 'String' or \n",
    "         data_type == 'Integer' or \n",
    "         data_type == 'Float') and unique_values < threshold):\n",
    "        return 'Categorical'\n",
    "    elif (data_type == 'Int' or data_type == 'Float'):\n",
    "        return 'Numerical'\n",
    "    else: \n",
    "        return 'Alphanumerical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative Characteristics | Nominal, Interval, Ordinal, Binary\n",
    "def f_qual(data_type):\n",
    "    if (data_type == 'String'):\n",
    "        return 'Nominal'\n",
    "    elif (data_type == 'Int' or data_type == 'Float'):\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Role | Input (Independant), ID (Unique Identifier)\n",
    "def f_role(f):\n",
    "    \n",
    "    f_unique_values = len(f.unique())\n",
    "    f_total_values = len(f)\n",
    "\n",
    "    if (f_unique_values / f_total_values) > 0.9:\n",
    "        return 'id'\n",
    "    else: \n",
    "        return 'input' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Profiling \n",
    "def f_profile(f):\n",
    "    \n",
    "    # Data Type\n",
    "    data_type = f_data_type(f)\n",
    "\n",
    "    # Role\n",
    "    role = f_role(f)\n",
    "\n",
    "    # Unique Values\n",
    "    unique_values = len(f.value_counts())\n",
    "\n",
    "    # Feature Type\n",
    "    feature_type = f_feature_type(data_type, unique_values, 10)\n",
    "\n",
    "    return {\n",
    "        'data_type' : data_type,\n",
    "        'role' : role,\n",
    "        'unique_values' : unique_values,\n",
    "        'feature_type' : feature_type\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All feature profiling\n",
    "def df_profile(df, f_target):\n",
    "    df_profile = {}\n",
    "\n",
    "    # Dataset\n",
    "    df_profile['dataset'] = df_shape(df)\n",
    "\n",
    "    # Target Feature\n",
    "    df_profile['target_feature'] = f_target\n",
    "\n",
    "    # Features\n",
    "    df_features = {}\n",
    "    for f in df.columns: \n",
    "        df_features[f] = f_profile(df[f])\n",
    "    df_profile['features'] = df_features\n",
    "\n",
    "    return df_profile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Profiling | Samples Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# House Prices\n",
    "hp_profiling = df_profile(df_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modcloth\n",
    "md_profiling = df_profile(df_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titanic \n",
    "tn_profile = df_profile(df_tn, tn_objective['target_feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis & Visualization | Descriptive Statistics\n",
    "\n",
    "### Dataset\n",
    "- Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Duplicates | Check if they exist\n",
    "def df_duplicates(df):\n",
    "    return {\n",
    "        'exist' : df.duplicated().any(),\n",
    "        'sum' : df.duplicated().sum()\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis \n",
    "\n",
    "- Interval, Ordinal Statistics | Count, Mean,  Std, Min, 25%, 50%, 75%, Max\n",
    "- Missing Values \n",
    "- Outliers\n",
    "- Histogram\n",
    "- Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "def f_statistics(f, d_type, f_type):\n",
    "\n",
    "    f_statistics = {}\n",
    "    if f_type == 'Numerical' and d_type != 'String':\n",
    "\n",
    "        descr = f.describe()\n",
    "        f_statistics = {\n",
    "            'count' : int(descr['count']),\n",
    "            'mean' : round(descr['mean'], 2),\n",
    "            'std' : round(descr['std'], 2),\n",
    "            'min' : round(descr['min'], 2),\n",
    "            'max' : round(descr['max'], 2)\n",
    "        }\n",
    "    elif f_type == 'Categorical': \n",
    "        for i, y in f.value_counts().items():\n",
    "            f_statistics[i] = y\n",
    "\n",
    "    return f_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values | Return: {Total Missing Values, Percentage}\n",
    "def f_missing_data(f):\n",
    "\n",
    "    ## Missing Values\n",
    "    missing_values = f.isnull().sum()\n",
    "\n",
    "    ## Percentage\n",
    "    percentage = round((missing_values / len(f)), 2)\n",
    "    return {\n",
    "        'missing_values': missing_values,\n",
    "        'percentage': percentage\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers | Univariate, Bivariate\n",
    "def f_outliers (f):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Univariate | Feature | {Dataframe_Feature, Data Type, Feature Type}\n",
    "def f_univariate(f, d_type, f_type): \n",
    "    f_univariate = {}\n",
    "\n",
    "    # Statistics\n",
    "    f_univariate['statistics'] = f_statistics(f, d_type, f_type)\n",
    "\n",
    "    # Missing Values\n",
    "    f_univariate['missing_data'] = f_missing_data(f)\n",
    "\n",
    "    return f_univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate | Dataframe | {Dataframe, Dataframe_Profiling}\n",
    "def df_univariate(df, df_prof):\n",
    "\n",
    "    df_univariate = {}\n",
    "    for f, d in df_prof['features'].items():\n",
    "        df_univariate[f] = f_univariate(df[f], d['data_type'], d['feature_type'])\n",
    "\n",
    "    df_univariate['features'] = df_univariate\n",
    "\n",
    "    return df_univariate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis\n",
    " - How each variable correlates to target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis | Each feature with target variable\n",
    "def f_corr (f, corr_matrix):\n",
    "    if f in corr_matrix:\n",
    "        return round(corr_matrix[f], 2)\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Analysis | Dataframe\n",
    "def df_bivariate(df, df_prof):\n",
    "\n",
    "    df_bivariate = {}\n",
    "\n",
    "    # Correlation Analysis\n",
    "    corr_matrix = df.corr(numeric_only=True)[df_prof['target_feature']]\n",
    "    for f in df_prof['features']:\n",
    "        df_bivariate[f] = f_corr(f, corr_matrix)\n",
    "    \n",
    "    return df_bivariate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Analysis\n",
    "- Normality -> Data should look like normal distribution\n",
    "- Homoscedasticity -> \n",
    "- Linearity -> Linear Patterns\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA  | {Datframe, Dataframe Profiling}\n",
    "def df_eda (df, df_prof):\n",
    "\n",
    "    df_eda = {}\n",
    "\n",
    "    # Duplicates\n",
    "    df_eda['duplicates'] = df_duplicates(df)\n",
    "\n",
    "    # Univariate\n",
    "    df_eda['univariate'] = df_univariate(df, df_prof)\n",
    "\n",
    "    # Bivariate\n",
    "    df_eda['bivariate'] = df_bivariate(df, df_prof)\n",
    " \n",
    "    return  df_eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics | Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titanic\n",
    "tn_eda = df_eda(df_tn, tn_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'features': 12, 'rows': 891},\n",
       " 'target_feature': 'Survived',\n",
       " 'features': {'PassengerId': {'data_type': 'Integer',\n",
       "   'role': 'id',\n",
       "   'unique_values': 891,\n",
       "   'feature_type': 'Alphanumerical'},\n",
       "  'Survived': {'data_type': 'Integer',\n",
       "   'role': 'input',\n",
       "   'unique_values': 2,\n",
       "   'feature_type': 'Categorical'},\n",
       "  'Pclass': {'data_type': 'Integer',\n",
       "   'role': 'input',\n",
       "   'unique_values': 3,\n",
       "   'feature_type': 'Categorical'},\n",
       "  'Name': {'data_type': 'String',\n",
       "   'role': 'id',\n",
       "   'unique_values': 891,\n",
       "   'feature_type': 'Alphanumerical'},\n",
       "  'Sex': {'data_type': 'String',\n",
       "   'role': 'input',\n",
       "   'unique_values': 2,\n",
       "   'feature_type': 'Categorical'},\n",
       "  'Age': {'data_type': 'Float',\n",
       "   'role': 'input',\n",
       "   'unique_values': 88,\n",
       "   'feature_type': 'Numerical'},\n",
       "  'SibSp': {'data_type': 'Integer',\n",
       "   'role': 'input',\n",
       "   'unique_values': 7,\n",
       "   'feature_type': 'Categorical'},\n",
       "  'Parch': {'data_type': 'Integer',\n",
       "   'role': 'input',\n",
       "   'unique_values': 7,\n",
       "   'feature_type': 'Categorical'},\n",
       "  'Ticket': {'data_type': 'String',\n",
       "   'role': 'input',\n",
       "   'unique_values': 681,\n",
       "   'feature_type': 'Alphanumerical'},\n",
       "  'Fare': {'data_type': 'Float',\n",
       "   'role': 'input',\n",
       "   'unique_values': 248,\n",
       "   'feature_type': 'Numerical'},\n",
       "  'Cabin': {'data_type': 'String',\n",
       "   'role': 'input',\n",
       "   'unique_values': 147,\n",
       "   'feature_type': 'Alphanumerical'},\n",
       "  'Embarked': {'data_type': 'String',\n",
       "   'role': 'input',\n",
       "   'unique_values': 3,\n",
       "   'feature_type': 'Categorical'}}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duplicates': {'exist': False, 'sum': 0},\n",
       " 'univariate': {'PassengerId': {'statistics': {},\n",
       "   'missing_data': {'missing_values': 0, 'percentage': 0.0}},\n",
       "  'Survived': {'statistics': {0: 549, 1: 342},\n",
       "   'missing_data': {'missing_values': 0, 'percentage': 0.0}},\n",
       "  'Pclass': {'statistics': {3: 491, 1: 216, 2: 184},\n",
       "   'missing_data': {'missing_values': 0, 'percentage': 0.0}},\n",
       "  'Name': {'statistics': {},\n",
       "   'missing_data': {'missing_values': 0, 'percentage': 0.0}},\n",
       "  'Sex': {'statistics': {'male': 577, 'female': 314},\n",
       "   'missing_data': {'missing_values': 0, 'percentage': 0.0}},\n",
       "  'Age': {'statistics': {'count': 714,\n",
       "    'mean': 29.7,\n",
       "    'std': 14.53,\n",
       "    'min': 0.42,\n",
       "    'max': 80.0},\n",
       "   'missing_data': {'missing_values': 177, 'percentage': 0.2}},\n",
       "  'SibSp': {'statistics': {0: 608, 1: 209, 2: 28, 4: 18, 3: 16, 8: 7, 5: 5},\n",
       "   'missing_data': {'missing_values': 0, 'percentage': 0.0}},\n",
       "  'Parch': {'statistics': {0: 678, 1: 118, 2: 80, 5: 5, 3: 5, 4: 4, 6: 1},\n",
       "   'missing_data': {'missing_values': 0, 'percentage': 0.0}},\n",
       "  'Ticket': {'statistics': {},\n",
       "   'missing_data': {'missing_values': 0, 'percentage': 0.0}},\n",
       "  'Fare': {'statistics': {'count': 891,\n",
       "    'mean': 32.2,\n",
       "    'std': 49.69,\n",
       "    'min': 0.0,\n",
       "    'max': 512.33},\n",
       "   'missing_data': {'missing_values': 0, 'percentage': 0.0}},\n",
       "  'Cabin': {'statistics': {},\n",
       "   'missing_data': {'missing_values': 687, 'percentage': 0.77}},\n",
       "  'Embarked': {'statistics': {'S': 644, 'C': 168, 'Q': 77},\n",
       "   'missing_data': {'missing_values': 2, 'percentage': 0.0}},\n",
       "  'features': {...}},\n",
       " 'bivariate': {'PassengerId': -0.01,\n",
       "  'Survived': 1.0,\n",
       "  'Pclass': -0.34,\n",
       "  'Name': '',\n",
       "  'Sex': '',\n",
       "  'Age': -0.08,\n",
       "  'SibSp': -0.04,\n",
       "  'Parch': 0.08,\n",
       "  'Ticket': '',\n",
       "  'Fare': 0.26,\n",
       "  'Cabin': '',\n",
       "  'Embarked': ''}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn_eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Correcting & Formatting & Completing (Transform)\n",
    "\n",
    "### Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicates\n",
    "def drop_duplicates(df): \n",
    "    try: \n",
    "        df.drop_duplicates(inplace=True)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imputation ~ Handle Missing Values\n",
    "\n",
    "Methods: Mean, Median, KNN, Most Frequent Value, Random Numbers between mean & std, Exploit correlated feature(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Missing Values | {Dataframe, Feature, Profile, EDA}\n",
    "\n",
    "## Check if drop or not\n",
    "def f_drop_feature(f_eda):\n",
    "\n",
    "    missing_percentage = f_eda['f_missing_data']['percentage']\n",
    "\n",
    "    if (missing_percentage > 0.9):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def df_handle_missing_values(df, f, f_profile, f_eda):\n",
    "\n",
    "    ## Check | If 10% of values is missing drop featureA\n",
    "\n",
    "    # Remove\n",
    "    if (f_drop_feature(f_eda)):\n",
    "        f.drop()  \n",
    "\n",
    "    # Mean | Numerical\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "    # Mode | Categorical\n",
    "\n",
    "    # Correlated Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Outliers\n",
    "def f_handle_outliers(f):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imputation Types | Simple, Iterative\n",
    "\n",
    "## Numeric | Drop, Mean, Median, Mode, KNN, Interpolated, Most Frequent\n",
    "# df_hp.fillna(df_hp.mean(), inplace=True)\n",
    "\n",
    "## Categorical | Drop, Mean, Median, Mode, KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data(df_hp, feature_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(duplicates_exists(df_hp))\n",
    "print(duplicates_exists(df_wasabi))\n",
    "print(drop_duplicates(df_hp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "- Feature Extraction\n",
    "- Feature Selection | Either in data-preprocessing, or the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Selection | Remove features with low or zero variance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
